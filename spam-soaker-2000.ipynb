{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Soaker 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have affectionately dubbed this spam filter, the Spam Soaker 2000. What it lacks in real world applicability, it more than makes up for with it's super cool name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Building a Spam Filter with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be building a spam filter using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "The data was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the <a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\">UCI Machine Learning Repository</a>. The data collection process is described in more details on <a href=\"http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition\">this page</a>, where you can also find some of the authors' papers.\n",
    "\n",
    "Let's begin with reading in this dataset and getting familiar with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  \\\n",
       "0      ham   \n",
       "1      ham   \n",
       "2     spam   \n",
       "3      ham   \n",
       "4      ham   \n",
       "...    ...   \n",
       "5567  spam   \n",
       "5568   ham   \n",
       "5569   ham   \n",
       "5570   ham   \n",
       "5571   ham   \n",
       "\n",
       "                                                                                                                                                                   SMS  \n",
       "0                                                      Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                        Ok lar... Joking wif u oni...  \n",
       "2          Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                                    U dun say so early hor... U c already then say...  \n",
       "4                                                                                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "...                                                                                                                                                                ...  \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.  \n",
       "5568                                                                                                                              Will ü b going to esplanade fr home?  \n",
       "5569                                                                                                         Pity, * was in mood for that. So...any other suggestions?  \n",
       "5570                                     The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free  \n",
       "5571                                                                                                                                        Rofl. Its true to its name  \n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5572 text messages, which is actually 2 less than expected, but this shouldn't make too much difference. We can also see that around 86.6% of messages in the data have been clasified as non-spam (referred to as 'ham' in the data), while 13.4% have been classified as spam- this seems like a reasonable proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split our data into training and testing. We will go for a training - testing split of 80 - 20, so 4458 messages will be in the training data, leaving 1114 in the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we first randomize the dataframe, then split it to avoid overlap\n",
    "training_data = data.sample(frac=1, random_state=1)[:4458]\n",
    "testing_data = data.sample(frac=1, random_state=1)[4458:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a little sanity check, let's verify that the percentages of spam and non-spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looking nice and random. Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick and Dirty Naive Bayes Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training our model, let's recap the algorithm we will be using- multinomial Naive Bayes. Firstly, we will define events:\n",
    "\n",
    "$Spam = \\text {event that message is spam}$\n",
    "<br>\n",
    "$w_i = \\text {event that message contains word } w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to use the training data to estimate some probabilities of some events, which we require to feed into Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Spam) = \\text{proportion of spam messages in training data}$\n",
    "\n",
    "$P(Ham) = \\text{proportion of ham messages in training data}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_i\\ |\\ Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}$\n",
    "\n",
    "$P(w_i\\ |\\ Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}$\n",
    "<br>\n",
    "<br>\n",
    "where:\n",
    "<br>\n",
    "<br>\n",
    "$N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages}$\n",
    "$N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages}$\n",
    "<br>\n",
    "<br>\n",
    "$N_{Spam} = \\text{total number of words in spam messages}$\n",
    "$N_{Spam^C} = \\text{total number of words in non-spam messages}$\n",
    "<br>\n",
    "<br>\n",
    "$N_{Vocabulary} = \\text{total number of words in the vocabulary}$\n",
    "\n",
    "$\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Ignoring the $\\alpha$ terms in the last two expressions, you can see we qutie simply just estimate these based on their relative proportions in the training data\n",
    "\n",
    "(note: the terms containing $\\alpha$ are used for **additive smoothing**- they prevent these probabilities being 0 when using a word which does not occur in the training data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a particular message we want to classify, containing words $w_1,w_2, ..., w_n$, we need to calculate the following two probabilities:\n",
    "\n",
    "$P(Spam\\ |\\ w_1,w_2, ..., w_n)$\n",
    "\n",
    "$P(Ham\\ |\\ w_1,w_2, ..., w_n)$\n",
    "\n",
    "Then, depending on which probability is larger, our model will classify this message accordingly. Let's now calculate these probabilities.\n",
    "\n",
    "According to Bayes' Theorem, we have:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Spam\\ |\\ w_1,w_2, ..., w_n) = \\frac{P(Spam) \\cdot P(w_1,w_2, ..., w_n\\ |\\  Spam)}{P(w_1,w_2, ..., w_n)}$\n",
    "\n",
    "$P(Ham\\ |\\ w_1,w_2, ..., w_n) = \\frac{P(Ham) \\cdot P(w_1,w_2, ..., w_n\\ |\\  Ham)}{P(w_1,w_2, ..., w_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, recall we only care about which of these two values is larger, so for efficiency, we can ignore the divisor $w_1,w_2, ..., w_n$, so let's rewrite this as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Spam\\ |\\ w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot P(w_1,w_2, ..., w_n\\ |\\  Spam)$\n",
    "\n",
    "$P(Ham\\ |\\ w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot P(w_1,w_2, ..., w_n\\ |\\  Ham)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to calculate:\n",
    "\n",
    "$P(w_1,w_2, ..., w_n\\ |\\  Spam)$\n",
    "\n",
    "$P(w_1,w_2, ..., w_n\\ |\\  Ham)$\n",
    "\n",
    "We make the (wildly unrealistic) assumption, that the events $w_i$ have conditional independence, and thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_1,w_2, ..., w_n\\ |\\  Spam) =  \\prod_{i=1}^{n}P(w_i\\ |\\ Spam)$\n",
    "\n",
    "$P(w_1,w_2, ..., w_n\\ |\\  Ham) =  \\prod_{i=1}^{n}P(w_i\\ |\\ Ham)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Spam\\ |\\ w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i\\ |\\ Spam)$\n",
    "\n",
    "$P(Ham\\ |\\ w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i\\ |\\ Ham)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's clear under the assumption of conditional independence, the probabilities we estiamte using our training data are enough to classify new messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some quick data cleaning. Our first step is to remove all punctuation, and make all words lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['SMS'] = training_data['SMS'].str.replace('\\W',' ').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to collect the unique set of words which occur in the training data, otherwise known as the **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "\n",
    "# make word list from each SMS\n",
    "training_data['SMS'] = training_data['SMS'].str.split()\n",
    "\n",
    "# aggregate all word lists into one\n",
    "for sms in training_data['SMS']:\n",
    "    vocabulary += sms\n",
    "    \n",
    "# remove dupes\n",
    "vocabulary = list(set(vocabulary))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use some clever logic to find how many occurrences of each word occur in each SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_data['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_data['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([training_data, word_counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun begins, let's create our spam filter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataframe into spam and ham\n",
    "spam = training_data[training_data['Label'] == 'spam']\n",
    "ham = training_data[training_data['Label'] == 'ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the probability of spam or ham (our prior probabilities for Bayes' Theorem)\n",
    "p_spam = len(spam) / 4458\n",
    "p_ham = len(ham) / 4458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total number of words in spam messages\n",
    "n_spam = 0\n",
    "\n",
    "for sms in spam['SMS']:\n",
    "    len_sms = len(sms)\n",
    "    n_spam += len_sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total number of words in ham messages\n",
    "n_ham = 0\n",
    "\n",
    "for sms in ham['SMS']:\n",
    "    len_sms = len(sms)\n",
    "    n_ham += len_sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of unique words in training data\n",
    "n_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize two dictionaries to store probabilities\n",
    "spam_dict = {word : 0 for word in vocabulary}\n",
    "ham_dict = {word : 0 for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all words in spam and ham messages\n",
    "\n",
    "spam_words = []\n",
    "for sms in spam['SMS']:\n",
    "    spam_words += sms\n",
    "    \n",
    "ham_words = []\n",
    "for sms in ham['SMS']:\n",
    "    ham_words += sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to calculate $P(w_i\\ |\\ Spam)$ for all words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unique_word in vocabulary:\n",
    "    # calculating how manay times each word in vocabulary occurs in spam and ham messages\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for word in spam_words:\n",
    "        if word == unique_word:\n",
    "            spam_count += 1\n",
    "    \n",
    "    for word in ham_words:\n",
    "        if word == unique_word:\n",
    "            ham_count += 1\n",
    "            \n",
    "    # calculate probability that word is spam / ham given it contains the word\n",
    "    p_word_spam = (spam_count + alpha) / (n_spam + alpha * n_vocabulary)\n",
    "    p_word_ham = (ham_count + alpha) / (n_ham + alpha * n_vocabulary)\n",
    "    \n",
    "    # finally update dictionaries with their respective probabilities\n",
    "    spam_dict[unique_word] = p_word_spam\n",
    "    ham_dict[unique_word] = p_word_ham\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our spam filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    # we need to calculate the probabilities that the message is spam given message\n",
    "    p_spam_given_message = p_spam\n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            prob = spam_dict[word]\n",
    "        else:\n",
    "            prob = alpha / (n_spam + alpha * n_vocabulary)\n",
    "            \n",
    "        p_spam_given_message *= prob\n",
    "     \n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            prob = ham_dict[word]\n",
    "        else:\n",
    "            prob = alpha / (n_ham + alpha * n_vocabulary)\n",
    "            \n",
    "        p_ham_given_message *= prob\n",
    "    \n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have built our spam classifier! Let's try it on a few simple messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.2186187832944059e-25\n",
      "P(Ham|message): 8.604237681688223e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify('Sounds good, Alex, then see u there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 3.2592569170008664e-24\n",
      "P(Ham|message): 4.578068659141562e-28\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('YOU WIN THE PRIZE MONEY JACKPOT! CALL 14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With obvious spam and non-spam, the classifier seems to be working in the way we would expect. Let's properly evaluate model performance using test data now. We just need to update our function to actual return something first, rather than print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    # we need to calculate the probabilities that the message is spam given message\n",
    "    p_spam_given_message = p_spam\n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            prob = spam_dict[word]\n",
    "        else:\n",
    "            prob = alpha / (n_spam + alpha * n_vocabulary)\n",
    "            \n",
    "        p_spam_given_message *= prob\n",
    "     \n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            prob = ham_dict[word]\n",
    "        else:\n",
    "            prob = alpha / (n_ham + alpha * n_vocabulary)\n",
    "            \n",
    "        p_ham_given_message *= prob\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['predicted'] = testing_data['SMS'].apply(classify_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2131</td>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3418</td>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3424</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Orange camera/video phones for FREE. Save £s with Free texts/weekend calls. Text YES for a callback orno to opt out</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult to type</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5393</td>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>ham</td>\n",
       "      <td>We're all getting worried over here, derek and taylor have already assumed the worst</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5192</td>\n",
       "      <td>ham</td>\n",
       "      <td>Oh oh... Den muz change plan liao... Go back have to yan jiu again...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>ham</td>\n",
       "      <td>CERI U REBEL! SWEET DREAMZ ME LITTLE BUDDY!! C YA 2MORO! WHO NEEDS BLOKES</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>spam</td>\n",
       "      <td>Text &amp; meet someone sexy today. U can find a date or even flirt its up to U. Join 4 just 10p. REPLY with NAME &amp; AGE eg Sam 25. 18 -msg recd@thirtyeight pence</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5157</td>\n",
       "      <td>ham</td>\n",
       "      <td>K k:) sms chat with me.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1114 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  \\\n",
       "2131   ham   \n",
       "3418   ham   \n",
       "3424  spam   \n",
       "1538   ham   \n",
       "5393   ham   \n",
       "...    ...   \n",
       "905    ham   \n",
       "5192   ham   \n",
       "3980   ham   \n",
       "235   spam   \n",
       "5157   ham   \n",
       "\n",
       "                                                                                                                                                                SMS  \\\n",
       "2131                                                                                                                      Later i guess. I needa do mcat study too.   \n",
       "3418                                                                                                                         But i haf enuff space got like 4 mb...   \n",
       "3424  Had your mobile 10 mths? Update to latest Orange camera/video phones for FREE. Save £s with Free texts/weekend calls. Text YES for a callback orno to opt out   \n",
       "1538                                                                                                          All sounds good. Fingers . Makes it difficult to type   \n",
       "5393                                                       All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!   \n",
       "...                                                                                                                                                             ...   \n",
       "905                                                                            We're all getting worried over here, derek and taylor have already assumed the worst   \n",
       "5192                                                                                          Oh oh... Den muz change plan liao... Go back have to yan jiu again...   \n",
       "3980                                                                                      CERI U REBEL! SWEET DREAMZ ME LITTLE BUDDY!! C YA 2MORO! WHO NEEDS BLOKES   \n",
       "235   Text & meet someone sexy today. U can find a date or even flirt its up to U. Join 4 just 10p. REPLY with NAME & AGE eg Sam 25. 18 -msg recd@thirtyeight pence   \n",
       "5157                                                                                                                                        K k:) sms chat with me.   \n",
       "\n",
       "     predicted  \n",
       "2131       ham  \n",
       "3418       ham  \n",
       "3424      spam  \n",
       "1538       ham  \n",
       "5393       ham  \n",
       "...        ...  \n",
       "905        ham  \n",
       "5192       ham  \n",
       "3980       ham  \n",
       "235       spam  \n",
       "5157       ham  \n",
       "\n",
       "[1114 rows x 3 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1114\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in testing_data.iterrows():\n",
    "    actual_classifcation = row[1]['Label']\n",
    "    predicted_classifcation = row[1]['predicted']\n",
    "    if actual_classifcation == predicted_classifcation:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981149012567325\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the Naive Bayes filter is suprisingly good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
